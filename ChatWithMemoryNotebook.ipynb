{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download Ollama, either from the command line or through the website: https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recoomended that you create and use a virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requests package is also needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the needed import statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the base URL of your local LLM server. This is the URL for Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function to pull the models from the Ollama server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_model(model_name):\n",
    "    \"\"\"\n",
    "    Pulls the specified model from the library to the local server.\n",
    "    \"\"\"\n",
    "    pull_url = f\"{BASE_URL}/api/pull\"\n",
    "    payload = {\"name\": model_name}\n",
    "    \n",
    "    # POST request to pull the model\n",
    "    response = requests.post(pull_url, json=payload)\n",
    "    \n",
    "    # Handle the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Model pull initiated.\")\n",
    "        # Streaming is true by default, so we expect multiple JSON objects\n",
    "        for line in response.iter_lines():\n",
    "            if line:  # filter out keep-alive new lines\n",
    "                print(json.loads(line.decode('utf-8')))\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can select the models you wish to install and use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the models\n",
    "#pull_model('mistral:7b')\n",
    "#pull_model('mistral:instruct')\n",
    "#pull_model('mistral:text')\n",
    "#pull_model('wizardlm2:7b')\n",
    "#pull_model('gemma:7b')\n",
    "#pull_model('llama2:13b')\n",
    "#pull_model('llama2:70b')\n",
    "pull_model('llama3:8b')\n",
    "#pull_model('llama3:instruct')\n",
    "pull_model('llama3:text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of how to customize and tweak the models with Modelfiles. The below builds the mario example from the Ollama documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama create mario -f ./Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a LLaMA3 model modified with a Modelfile to have a lowered temperature for a more coherent and less creative response type and a custom system prompt to enhance its focus on the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama create summary_llama3 -f ./Modelfile_Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the models you wish to use for which functions. Currently there are two: The LLM that you wish to interact with, and the LLM that will provide the conversation summary to the conversational LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see your installed and created models with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code deletes models that are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama rm summary_llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name\n",
    "MODEL_NAME = 'llama3:8b'\n",
    "SUMMARY_MODEL_NAME = 'summary_llama3'\n",
    "#SUMMARY_MODEL_NAME = 'llama3:text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function initializes the memory file if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memory(file_path):\n",
    "    \"\"\"Initialize the memory file if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump([], file)  # Initialize with an empty list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions read from and write to the memory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_memory(file_path, person):\n",
    "    \"\"\"Retrieve a specific person's interaction from the memory.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for interaction in data:\n",
    "            if interaction['person'] == person:\n",
    "                return interaction['conversation']\n",
    "    return None\n",
    "\n",
    "def write_memory(file_path, person, user_input, llm_response):\n",
    "    \"\"\"Write or update a person's interaction in the memory.\"\"\"\n",
    "    conversation = f\"You: {user_input} LLM: {llm_response}\"\n",
    "    found = False\n",
    "    with open(file_path, 'r+') as file:\n",
    "        data = json.load(file)\n",
    "        for interaction in data:\n",
    "            if interaction['person'] == person:\n",
    "                interaction['conversation'] += f\" {conversation}\"  # Append new conversation\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            data.append({'person': person, 'conversation': conversation})\n",
    "        file.seek(0)\n",
    "        file.truncate()\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function handles querying the chosen conversational LLM and recieving the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(prompt, model_name=MODEL_NAME, stream=True, context=None, conversation_history=None):\n",
    "    \"\"\"\n",
    "    This function sends a prompt to the local LLM and returns the response and context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate conversation history, context, and prompt correctly\n",
    "    if conversation_history:\n",
    "        full_prompt = f\"{conversation_history}\\n{context}\\n{prompt}\"\n",
    "    else:\n",
    "        full_prompt = f\"{context}\\n{prompt}\" if context else prompt\n",
    "\n",
    "    generate_url = f\"{BASE_URL}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": full_prompt,\n",
    "        \"stream\": stream,  # We are explicitly asking for streaming responses\n",
    "        \"context\": context,  # Include context if available\n",
    "        \"options\": {\"num_ctx\": 4096}\n",
    "    }\n",
    "    \n",
    "    # POST request to the LLM\n",
    "    response = requests.post(generate_url, json=payload, stream=stream)\n",
    "    full_response = \"\"  # Initialize to capture full response\n",
    "    next_context = None  # Initialize context to None\n",
    "    \n",
    "    # Handle the response\n",
    "    if response.status_code == 200:\n",
    "        if not stream:\n",
    "            # If not streaming, we expect one JSON object\n",
    "            response_data = response.json()\n",
    "            full_response = response_data.get('response', '')\n",
    "            next_context = response_data.get('context')\n",
    "        else:\n",
    "            # If streaming, handle the streaming response\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    response_data = json.loads(line.decode('utf-8'))\n",
    "                    if 'response' in response_data:\n",
    "                        full_response += response_data['response']  # Accumulate the response\n",
    "                    if 'context' in response_data:  # Capture the context if present\n",
    "                        next_context = response_data['context']\n",
    "                    if response_data.get('done'):  # Check if it's the end of the response\n",
    "                        break\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "    \n",
    "    return next_context, full_response  # Return the context and full response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses the Summary LLM to provide a concise summary of the conversation history when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(prompt, summary_model_name=SUMMARY_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    This function sends a prompt to the summarization LLM and returns the summary.\n",
    "    \"\"\"\n",
    "    summary_instruction = \"Please provide a concise summary of the following conversation: \"\n",
    "    summary_prompt = f\"{summary_instruction} {prompt}\"\n",
    "\n",
    "\n",
    "    generate_url = f\"{BASE_URL}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": summary_model_name,\n",
    "        \"prompt\": summary_prompt,\n",
    "        # Assuming that summarization also requires streaming\n",
    "        \"stream\": True,  # Update this based on whether the model supports streaming\n",
    "    }\n",
    "    \n",
    "    # POST request to the summarization LLM\n",
    "    response = requests.post(generate_url, json=payload, stream=True)  # Assuming streaming is needed\n",
    "    full_summary = \"\"  # Initialize to capture full summary\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        for line in response.iter_lines():\n",
    "            if line:  # filter out keep-alive new lines\n",
    "                response_data = json.loads(line.decode('utf-8'))\n",
    "                if 'response' in response_data:\n",
    "                    full_summary += response_data['response']  # Accumulate the response\n",
    "                if response_data.get('done'):  # Check if it's the end of the response\n",
    "                    break\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "    \n",
    "    return full_summary  # Return the full summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the main driver for the Chat with Memory program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli_llm():\n",
    "    global context  # Use the global context variable\n",
    "    memory_file = 'memory.json'  # Define the path to the memory file\n",
    "    gen_model = MODEL_NAME\n",
    "    summary_model_name = SUMMARY_MODEL_NAME\n",
    "\n",
    "    init_memory(memory_file)  # Initialize memory file if it doesn't exist\n",
    "    print(\"Chat with Memory is running. You are chatting with: \" + MODEL_NAME + \". Type 'exit' to quit.\")\n",
    "    \n",
    "    user = input(\"Please enter your name to start: \")\n",
    "    conversation_history = read_memory(memory_file, user)\n",
    "    \n",
    "    if conversation_history:\n",
    "        print(f\"Welcome back, {user}! You have previous conversations recorded.\")\n",
    "        if input(\"Would you like a summary of your last conversation(s)? (yes/no): \").lower().strip() == 'yes':\n",
    "            summary = get_summary(conversation_history, summary_model_name)\n",
    "            print(\"Here is the summary of your last conversation(s):\")\n",
    "            print(summary)\n",
    "        else:\n",
    "            print(\"Continuing without a summary.\")\n",
    "        #print(\"Your previous conversation was:\")\n",
    "        #print(conversation_history)\n",
    "    else:\n",
    "        print(f\"Hello, {user}! Starting a new conversation.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        print(f\"You: {user_input}\")  # Echo the user input\n",
    "        context, response = generate_completion(\n",
    "            prompt=user_input, \n",
    "            model_name=gen_model,  # Make sure gen_model is defined, or use MODEL_NAME directly\n",
    "            stream=True, \n",
    "            context=context, \n",
    "            conversation_history=conversation_history\n",
    ")\n",
    "        print(f\"LLM: {response}\")  # Print LLM's response\n",
    "        write_memory(memory_file, user, user_input, response)  # Update memory with the new input and response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This launches the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to start the command line interface\n",
    "context = None\n",
    "cli_llm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
